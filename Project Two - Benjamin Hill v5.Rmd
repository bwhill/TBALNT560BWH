---
title: "Project Two"
author: "Benjamin Hill"
date: "`r format(Sys.time(), '%B, %d, %Y')`"
output:
  pdf_document: 
    toc: yes
    toc_depth: 3
    extra_dependencies: ["float"]
  word_document: default
  html_document: 
    df_print: paged
fig.caption: yes
keep_tex: yes
always_allow_html: true
mainfont: Palatino
header-includes: 
  \usepackage{dcolumn}
  \usepackage{longtable}
  \usepackage{float}
  \usepackage{rotating}
  \floatplacement{figure}{H}
---

\newpage

```{r knitrcode, echo=TRUE, message=FALSE, warning=FALSE}
# This code sets options for when we knit the rmarkdown to a PDF or other doc
knitr::opts_chunk$set(include=TRUE,fig.align='center', comment=NA,fig.pos = '!H')
knitr::opts_knit$set(root.dir = normalizePath("/Users/ben/Documents/UWT/Classes/560 - Data Mining/Project Two Homework/"))
```

```{r instpackage, echo=TRUE, message=FALSE, warning=FALSE}
# keep track of packages that need to be installed in the environment
# install.packages("flextable")
# install.packages("officer")
# install.packages("tibble")
# install.packages("tidyverse")
# install.packages("docxtractr")
# install.packages("compare_df")
# install.packages("DataExplorer")
# install.packages("GPArotation")
# install.packages("psychTools")
# install.packages("PerformanceAnalytics")
# install.packages("hablar")
# install.packages("caretEnsemble")
# install.packages("pivottabler")
# install.packages("rattle")
# install.packages("gapminder")
```

```{r inclibs, echo=TRUE, message=FALSE, warning=FALSE}
# libraries to use - not all are used in the code
library(DataExplorer) 
library(xtable)
library(summarytools)
library(e1071)
library(gplots)
library(ggplot2)
library(caret)
library(tidyverse)
require(knitr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gapminder)
library(GGally)
```

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# default options for this rmarkdown file. 
# I will change these options on a chunk by chunk basis depending on what 
# I am trying to do

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
setwd("/Users/ben/Documents/UWT/Classes/560 - Data Mining/Project Two Homework/")

# clear previous variables and set random seed.
rm(list=ls()) 
set.seed(12345)
```

```{r fsetup, echo=TRUE, message=FALSE, warning=FALSE}
# define any functions used in the code
# the URL references are where I got the code from

# https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

```

# Data Exploration and Preparation

Developing a Data Mining Portfolio -  End to End Project â€“ Classification

This project involves generating several classification models for the same data set and then combining the output from the models in an ensemble fashion.    An R file is provided that generates classification models using the Iris data set.  You will want to adapt this for the BreastCancer data set which is provided.  You will want to add code that combines the output from the different models using Majority rule ensemble approach. 
## Load And Shape Data

Load the mlbench package which has the BreastCancer data set. Some algorithms don't like missing values, so remove rows with missing values. Remove the unique identifier, which is useless and would confuse the machine learning algorithms.

```{r readdata, echo=TRUE, message=FALSE, warning=FALSE}

#load the mlbench package which has the BreastCancer data set
require(mlbench)

# load the data set
data(BreastCancer)

# some algorithms don't like missing values, so remove rows with missing values
# remove the unique identifier, which is useless and would confuse the machine learning algorithms
# I generally use the tidyverse functions to do any data manipulations

plot_missing(BreastCancer, title="BreastCancer dataset missing values")

df <- BreastCancer %>%
  dplyr::select(-Id) %>%
  drop_na()

# the 'introduce' function is from the DataExplorer package
# https://rdrr.io/cran/DataExplorer/f/vignettes/dataexplorer-intro.Rmd
breastoverview <- as.data.frame(t(introduce(df)))

breastoverview %>%
  dplyr::rename(.,"Overview"="V1") 


```

# Exploratory Data Analysis

EDA completed on the breast cancer dataset. 

## Data Frame Summary

```{r dfsum, echo=TRUE, message=FALSE, warning=FALSE, results="asis"}
# from the summarytools package
# https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

dfSummary(df, plain.ascii = FALSE, style = "grid",
          graph.magnif = 0.75, valid.col = FALSE,
          tmp.img.dir = "/tmp")

#ctable(df)

freq(df, plain.ascii = FALSE, 
            headings = TRUE, 
            style = "rmarkdown") 


```

### Visual Data Exploration

```{r dxplor, echo=TRUE, message=FALSE, warning=FALSE}
# Using EDA functions from DataExplorer

DataExplorer::plot_bar(df, 
                       title="BreastCancer dataset",nrow=1L,ncol=2L)

DataExplorer::plot_bar(df,by="Class",
                       title="BreastCancer dataset",nrow=1L,ncol=2L)

# convert factor to numeric for EDA, but keep Class as a factor
df1 <- df %>%
  mutate(., Class = as.character(Class)) %>%
  mutate_if(., is.factor, ~ as.numeric(levels(.x))[.x]) %>%
  mutate(., Class = as.factor(Class))

DataExplorer::plot_histogram(df1, 
                             title="BreastCancer Histograms",nrow=2L,ncol=3L)


# Correlogram
df1 %>%
  dplyr::select(where(is.numeric)) %>%
  ggpairs(., title="BreastCancer Correlogram") 

```

## Cross-Tabulations

```{r ctblsum, echo=TRUE, message=FALSE, warning=FALSE, results="asis"}
# from the summarytools package

# Cross-Tabulations (joint frequencies) between pairs of discrete/categorical variables, featuring marginal sums as well as row, column or total proportions 

ctable(x = df$Cl.thickness, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Cell.size, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Cell.shape, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Marg.adhesion, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Epith.c.size, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Bare.nuclei, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Bl.cromatin, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Normal.nucleoli, y = df$Class, chisq = TRUE, prop = "r")
ctable(x = df$Normal.nucleoli, y = df$Mitoses, chisq = TRUE, prop = "r")

```


```{r dfsplit, echo=TRUE, message=FALSE, warning=FALSE}

# st the percentage of train here it is 0.8
percnt.of.data <- 0.8

# split data
train.index <- createDataPartition(y = df$Class ,
                                   p=percnt.of.data,
                                   list=FALSE)

# We now use the indexes set up to reference the rows we are sampling
train.df <- df[train.index, ]
valid.df <- df[-train.index, ]

```

# Model Development

All models are implemented via caret.

```{r startcore, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# I set this on my Mac to use multiple cores - speed up tree models later
library(doParallel)

cl <- makePSOCKcluster(6)

registerDoParallel(cl)

```

## Recursive Partitioning and Regression Tree

```{r xdt, echo=TRUE, message=FALSE, warning=FALSE}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 3)

x.rp <- train(Class ~ ., 
              data=train.df,
              method="rpart",
              trControl = ctrl)

#summary(x.rp)
rpart.plot(x.rp$finalModel)

# Make predictions - probabilities
x.rp.prob <- predict(x.rp, type="prob", valid.df)
# score the evaluation data set (extract the probabilities for graphing later)

# convert probabilities into factor for confusion matrix
x.rp.pred <- x.rp.prob %>% 
  as_tibble(.) %>%
  mutate(Class = if_else(benign >= malignant, "benign", "malignant")) %>%
  mutate(Class = as.factor(Class))

# Summarize results
confusionMatrix(x.rp.pred$Class,valid.df$Class)

```

## Support Vector Machine

```{r xsvm, echo=TRUE, message=FALSE, warning=FALSE}

# svm requires tuning
xsvm.tune <- tune(svm, Class~., 
                   data = train.df,
                   ranges = list(gamma = 2^(-8:1), 
                                 cost = 2^(0:4)),
                   tunecontrol = tune.control(sampling = "fix"))

# display the tuning results (in text format)
#x.svm.tune

# If the tuning results are on the margin of the parameters (e.g., gamma = 2^-8), then widen the parameters. I manually copied the cost and gamma from console messages above to parameters below.
x.svm <- svm(Class~.,
             data = train.df,
             cost=2, 
             gamma=0.25, 
             probability = TRUE)

x.svm.prob <- predict(x.svm, 
                      type="prob", 
                      newdata=valid.df, 
                      probability = TRUE)

confusionMatrix(x.svm.prob,valid.df$Class)
```

## Conditional Inference Tree

```{r xcit, echo=TRUE, message=FALSE, warning=FALSE}

# Leave-1-Out Cross Validation (LOOCV)
fit_control <- trainControl(method="LOOCV")

# create model using conditional inference trees

x.cit <- train(Class ~ ., 
              data=train.df,
              trControl = fit_control,
              method="ctree")
#summary(x.nb)

# predict classes for the evaluation data set
x.cit.pred <- predict(x.cit, valid.df)

confusionMatrix(x.cit.pred,valid.df$Class)

# To view the decision tree, uncomment this line.
plot(x.cit$finalModel, main="Decision tree created using condition inference trees")

# score the evaluation data set (extract the probabilities for graphing later)
x.cit.prob <- predict(x.cit, type="prob", newdata=valid.df)

```

## Conditional Inference Random Forest

```{r xrfbecit, echo=TRUE, message=FALSE, warning=FALSE}
# Train model 

x.rfbecit <- train(Class ~ ., 
              data=train.df,
              trControl = fit_control,
              method="cforest")
#summary(x.nb)

# predict classes for the evaluation data set
x.rfbecit.pred <- predict(x.rfbecit, valid.df)

confusionMatrix(x.rfbecit.pred,valid.df$Class)

# score the evaluation data set (extract the probabilities for graphing later)
x.rfbecit.prob <- predict(x.rfbecit, type="prob", newdata=valid.df)

```

## Bagged CART

```{r xrfbag, echo=TRUE, message=FALSE, warning=FALSE}

x.xrfbag2 <- train(Class ~ ., 
              data=train.df,
              trControl = fit_control,
              method="treebag")

x.ip.prob2 <- predict(x.xrfbag2, type="prob", newdata=valid.df)

# convert probabilities into factor for confusion matrix
x.ip.prob2 <- x.ip.prob2 %>% 
  as_tibble(.) %>%
  mutate(Class = if_else(benign >= malignant, "benign", "malignant")) %>%
  mutate(Class = as.factor(Class))

confusionMatrix(x.ip.prob2$Class,valid.df$Class)

# score the evaluation data set (extract the probabilities for graphing later)
x.xrfbag2.prob <- predict(x.xrfbag2, type="prob", newdata=valid.df)

```

# ROC Curves

```{r commpmodel, echo=TRUE, message=FALSE, warning=FALSE}

##
## plot ROC curves to compare the performance of the individual classifiers
## create an ROCR prediction objects from the various probabilities


# load the ROCR package which draws the ROC curves
require(ROCR)

x.rp.prob.rocr <- prediction(x.rp.prob[,2], valid.df['Class'])
# prepare an ROCR performance object for ROC curve (tpr=true positive rate, fpr=false positive rate)
x.rp.perf <- performance(x.rp.prob.rocr, "tpr","fpr")
# plot it
plot(x.rp.perf, col=2, main="ROC curves comparing classification performance of five machine learning models")

# Draw a legend.
legend(0.6, 0.6, c('rpart', 'svm', 'cit','cforest', 'rforest bagging'), 2:6)

# svm
x.svm.prob.rocr <- prediction(attr(x.svm.prob, "probabilities")[,2], valid.df['Class'])
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
plot(x.svm.perf, col=6, add=TRUE)

# ctree
x.cit.prob.rocr <- prediction(x.cit.prob[,2], valid.df['Class'])
x.cit.perf <- performance(x.cit.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart 
plot(x.cit.perf, col=3, add=TRUE)

# cforest
x.rfbecit.prob.rocr <- prediction(x.rfbecit.prob[,2], valid.df['Class'])
x.rfbecit.perf <- performance(x.rfbecit.prob.rocr, "tpr","fpr")
plot(x.rfbecit.perf, col=4, add=TRUE)

# bagging
x.ip.prob2.rocr <- prediction(x.ip.prob2[,2], valid.df['Class'])
x.ip.perf2 <- performance(x.ip.prob2.rocr, "tpr","fpr")
plot(x.ip.perf2, col=5, add=TRUE)


```


# Ensemble

'Manual' code that combines the output from the different models using Majority rule ensemble approach. 


```{r ensemblemodel, echo=TRUE, message=FALSE, warning=FALSE}

# build dataframe with all 
emodel <- cbind(x.svm.prob,
                x.rp.pred$Class,
                x.cit.pred,
                x.rfbecit.pred,
                x.ip.prob2$Class) 

# This uses the Mode function to calculate the mode across the columns
prvalue <- apply(emodel, 1, Mode)

# convert the output into a datafame and create factors
emodel <- as.data.frame(cbind(emodel,prvalue))
emodel$prvalue = as.factor(ifelse(emodel$prvalue == 1, "benign","malignant"))

# create confusion matrix for ensemble model
confusionMatrix(emodel$prvalue,valid.df$Class)

```


```{r buildtable, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# build a table comparing all the various model classification accuracy measures

otable <- cbind(confusionMatrix(x.rp.pred$Class,valid.df$Class)[["overall"]],
                confusionMatrix(x.svm.prob,valid.df$Class)[["overall"]],
                confusionMatrix(x.cit.pred,valid.df$Class)[["overall"]],
                confusionMatrix(x.rfbecit.pred,valid.df$Class)[["overall"]],
                confusionMatrix(x.ip.prob2$Class,valid.df$Class)[["overall"]],
                confusionMatrix(emodel$prvalue,valid.df$Class)[["overall"]])

ctable <- cbind(confusionMatrix(x.rp.pred$Class,valid.df$Class)[["byClass"]],
                confusionMatrix(x.svm.prob,valid.df$Class)[["byClass"]],
                confusionMatrix(x.cit.pred,valid.df$Class)[["byClass"]],
                confusionMatrix(x.rfbecit.pred,valid.df$Class)[["byClass"]],
                confusionMatrix(x.ip.prob2$Class,valid.df$Class)[["byClass"]],
                confusionMatrix(emodel$prvalue,valid.df$Class)[["byClass"]])

btable <-as.data.frame(rbind(otable,ctable)) %>%
  dplyr::rename("Recursive Partitioning and Regression Tree" = V1,
                "Support Vector Machine" = V2,
                "Conditional Inference Tree" = V3,
                "Conditional Inference Random Forest" = V4,
                "Bagged CART" = V5,
                "Majority Vote Ensemble" = V6)

print(xtable(btable,
       caption="Classification models inc ensemble model",
       digits=c(1,4,4,4,4,4,4)),
      caption.placement = "top", 
      floating=TRUE, 
      rotate.colnames = TRUE,
      size="\\fontsize{8pt}{10pt}\\selectfont",
      latex.environments = "center", 
      comment=FALSE)

# stop parallel cluster
# commented out for the rmarkdown version
#stopCluster(cl)  
```